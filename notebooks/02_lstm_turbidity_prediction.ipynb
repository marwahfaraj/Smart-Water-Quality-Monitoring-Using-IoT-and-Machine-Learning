{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Deep Learning Model for Turbidity Prediction\n",
        "\n",
        "## AAI-530 Final Project - Machine Learning Method 1\n",
        "\n",
        "This notebook implements an LSTM (Long Short-Term Memory) neural network built from scratch using TensorFlow/Keras to predict future water turbidity levels based on historical sensor data.\n",
        "\n",
        "**Objective**: Time series forecasting - Predict turbidity 24 hours ahead\n",
        "\n",
        "**Why LSTM?**\n",
        "- LSTMs excel at capturing long-term dependencies in sequential data\n",
        "- Water quality parameters exhibit temporal patterns (daily, seasonal)\n",
        "- Can learn complex non-linear relationships between multiple sensor inputs\n",
        "\n",
        "**Model Architecture**:\n",
        "- Built from scratch using TensorFlow/Keras\n",
        "- Multi-layer LSTM with dropout for regularization\n",
        "- Input: Sequences of historical sensor readings\n",
        "- Output: Predicted turbidity value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Deep Learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "print(\"Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data and preprocess (if processed data not available)\n",
        "DATA_DIR = '../archive'\n",
        "\n",
        "# Load all station data\n",
        "def load_all_stations(data_dir):\n",
        "    \"\"\"Load and combine data from all monitoring stations.\"\"\"\n",
        "    all_data = []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(data_dir, filename)\n",
        "            station_name = filename.replace('_joined.csv', '').replace('_', ' ').title()\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "            df['Station'] = station_name\n",
        "            all_data.append(df)\n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# Load data\n",
        "df = load_all_stations(DATA_DIR)\n",
        "print(f\"Loaded {len(df):,} records from {df['Station'].nunique()} stations\")\n",
        "print(f\"Date range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select station with most complete Turbidity data for LSTM training\n",
        "station_turbidity_counts = df.groupby('Station')['Turbidity'].apply(lambda x: x.notna().sum())\n",
        "best_station = station_turbidity_counts.idxmax()\n",
        "print(f\"Selected station for LSTM: {best_station}\")\n",
        "print(f\"Available Turbidity readings: {station_turbidity_counts[best_station]:,}\")\n",
        "\n",
        "# Filter to selected station\n",
        "df_station = df[df['Station'] == best_station].copy()\n",
        "df_station = df_station.sort_values('Timestamp').reset_index(drop=True)\n",
        "print(f\"\\nStation data shape: {df_station.shape}\")\n",
        "df_station.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing for LSTM\n",
        "# Select features for prediction\n",
        "feature_cols = ['Turbidity', 'Conductivity', 'Temp']\n",
        "available_features = [col for col in feature_cols if col in df_station.columns]\n",
        "print(f\"Available features: {available_features}\")\n",
        "\n",
        "# Filter to rows with Turbidity data (our target variable)\n",
        "df_lstm = df_station[['Timestamp'] + available_features].copy()\n",
        "df_lstm = df_lstm.dropna(subset=['Turbidity'])\n",
        "\n",
        "# Interpolate missing values in other features\n",
        "for col in available_features:\n",
        "    df_lstm[col] = df_lstm[col].interpolate(method='linear', limit=6)\n",
        "\n",
        "# Drop any remaining NaN rows\n",
        "df_lstm = df_lstm.dropna()\n",
        "df_lstm = df_lstm.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nLSTM dataset shape: {df_lstm.shape}\")\n",
        "print(f\"Date range: {df_lstm['Timestamp'].min()} to {df_lstm['Timestamp'].max()}\")\n",
        "df_lstm.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Scaling and Sequence Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features using MinMaxScaler\n",
        "# This is important for LSTM as it helps with gradient computation\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Get feature values (exclude timestamp)\n",
        "feature_data = df_lstm[available_features].values\n",
        "scaled_data = scaler.fit_transform(feature_data)\n",
        "\n",
        "print(f\"Scaled data shape: {scaled_data.shape}\")\n",
        "print(f\"Feature range after scaling: [{scaled_data.min():.4f}, {scaled_data.max():.4f}]\")\n",
        "\n",
        "# Get the index of Turbidity (our target variable)\n",
        "target_idx = available_features.index('Turbidity')\n",
        "print(f\"Target variable (Turbidity) index: {target_idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(data, target_col_idx, sequence_length=24, forecast_horizon=24):\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM model training.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : np.array\n",
        "        Scaled feature data\n",
        "    target_col_idx : int\n",
        "        Index of target column in data\n",
        "    sequence_length : int\n",
        "        Number of past time steps to use as input (lookback window)\n",
        "    forecast_horizon : int\n",
        "        Number of steps ahead to predict\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X : np.array\n",
        "        Input sequences of shape (samples, sequence_length, features)\n",
        "    y : np.array\n",
        "        Target values of shape (samples,)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    \n",
        "    for i in range(sequence_length, len(data) - forecast_horizon + 1):\n",
        "        # Input: sequence of past values\n",
        "        X.append(data[i - sequence_length:i])\n",
        "        # Output: target value at forecast_horizon steps ahead\n",
        "        y.append(data[i + forecast_horizon - 1, target_col_idx])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Hyperparameters\n",
        "SEQUENCE_LENGTH = 48  # Use 48 hours of historical data\n",
        "FORECAST_HORIZON = 24  # Predict 24 hours ahead\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(scaled_data, target_idx, SEQUENCE_LENGTH, FORECAST_HORIZON)\n",
        "\n",
        "print(f\"Input sequences shape (X): {X.shape}\")\n",
        "print(f\"  - Samples: {X.shape[0]}\")\n",
        "print(f\"  - Sequence length (time steps): {X.shape[1]}\")\n",
        "print(f\"  - Features: {X.shape[2]}\")\n",
        "print(f\"\\nTarget shape (y): {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training, validation, and test sets\n",
        "# For time series, we use sequential split (not random) to maintain temporal order\n",
        "\n",
        "train_size = int(len(X) * 0.7)\n",
        "val_size = int(len(X) * 0.15)\n",
        "\n",
        "X_train = X[:train_size]\n",
        "y_train = y[:train_size]\n",
        "\n",
        "X_val = X[train_size:train_size + val_size]\n",
        "y_val = y[train_size:train_size + val_size]\n",
        "\n",
        "X_test = X[train_size + val_size:]\n",
        "y_test = y[train_size + val_size:]\n",
        "\n",
        "print(\"Data Split Summary:\")\n",
        "print(f\"  Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build LSTM Model from Scratch\n",
        "\n",
        "Building a multi-layer LSTM neural network using TensorFlow/Keras. The architecture includes:\n",
        "- Input layer matching our sequence shape\n",
        "- Two LSTM layers with dropout for regularization\n",
        "- Dense output layer for prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lstm_model(sequence_length, n_features, units_1=64, units_2=32, dropout_rate=0.2):\n",
        "    \"\"\"\n",
        "    Build an LSTM model from scratch for time series prediction.\n",
        "    \n",
        "    Architecture:\n",
        "    - Input Layer: (sequence_length, n_features)\n",
        "    - LSTM Layer 1: units_1 neurons with return_sequences=True\n",
        "    - Dropout Layer 1: dropout_rate\n",
        "    - LSTM Layer 2: units_2 neurons\n",
        "    - Dropout Layer 2: dropout_rate\n",
        "    - Dense Output Layer: 1 neuron (prediction)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sequence_length : int\n",
        "        Number of time steps in input sequence\n",
        "    n_features : int\n",
        "        Number of features per time step\n",
        "    units_1 : int\n",
        "        Number of LSTM units in first layer\n",
        "    units_2 : int\n",
        "        Number of LSTM units in second layer\n",
        "    dropout_rate : float\n",
        "        Dropout rate for regularization\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    keras.Model\n",
        "        Compiled LSTM model\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # Input layer - explicitly define input shape\n",
        "        Input(shape=(sequence_length, n_features)),\n",
        "        \n",
        "        # First LSTM layer - returns sequences for stacking\n",
        "        LSTM(units=units_1, \n",
        "             return_sequences=True,\n",
        "             activation='tanh',\n",
        "             recurrent_activation='sigmoid',\n",
        "             name='lstm_layer_1'),\n",
        "        Dropout(dropout_rate, name='dropout_1'),\n",
        "        \n",
        "        # Second LSTM layer - returns final output only\n",
        "        LSTM(units=units_2,\n",
        "             return_sequences=False,\n",
        "             activation='tanh',\n",
        "             recurrent_activation='sigmoid',\n",
        "             name='lstm_layer_2'),\n",
        "        Dropout(dropout_rate, name='dropout_2'),\n",
        "        \n",
        "        # Dense hidden layer\n",
        "        Dense(units=16, activation='relu', name='dense_hidden'),\n",
        "        \n",
        "        # Output layer - single value prediction\n",
        "        Dense(units=1, activation='linear', name='output')\n",
        "    ])\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mse',  # Mean Squared Error for regression\n",
        "        metrics=['mae']  # Mean Absolute Error\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "n_features = X_train.shape[2]\n",
        "model = build_lstm_model(SEQUENCE_LENGTH, n_features, units_1=64, units_2=32, dropout_rate=0.2)\n",
        "\n",
        "# Display model architecture\n",
        "print(\"LSTM Model Architecture:\")\n",
        "print(\"=\" * 60)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define callbacks for training\n",
        "callbacks = [\n",
        "    # Early stopping to prevent overfitting\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Reduce learning rate when validation loss plateaus\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.0001,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Save best model\n",
        "    ModelCheckpoint(\n",
        "        filepath='../models/lstm_turbidity_model.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Final training loss: {history.history['loss'][-1]:.6f}\")\n",
        "print(f\"Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
        "axes[0].set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# MAE plot\n",
        "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('MAE', fontsize=12)\n",
        "axes[1].set_title('Mean Absolute Error During Training', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/lstm_training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred_scaled = model.predict(X_test, verbose=0)\n",
        "\n",
        "# Inverse transform predictions to original scale\n",
        "# We need to create a dummy array with the same shape as original features\n",
        "def inverse_transform_target(scaled_pred, scaler, target_idx, n_features):\n",
        "    \"\"\"Convert scaled predictions back to original scale.\"\"\"\n",
        "    # Create dummy array\n",
        "    dummy = np.zeros((len(scaled_pred), n_features))\n",
        "    dummy[:, target_idx] = scaled_pred.flatten()\n",
        "    \n",
        "    # Inverse transform\n",
        "    original = scaler.inverse_transform(dummy)\n",
        "    return original[:, target_idx]\n",
        "\n",
        "y_pred = inverse_transform_target(y_pred_scaled, scaler, target_idx, len(available_features))\n",
        "y_test_original = inverse_transform_target(y_test.reshape(-1, 1), scaler, target_idx, len(available_features))\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_original, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_original, y_pred)\n",
        "r2 = r2_score(y_test_original, y_pred)\n",
        "\n",
        "print(\"Model Evaluation Metrics (Test Set):\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Mean Squared Error (MSE):  {mse:.4f}\")\n",
        "print(f\"  Root MSE (RMSE):           {rmse:.4f} NTU\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae:.4f} NTU\")\n",
        "print(f\"  R² Score:                  {r2:.4f}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions vs actual values\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Scatter plot: Predicted vs Actual\n",
        "axes[0, 0].scatter(y_test_original, y_pred, alpha=0.5, s=20, c='steelblue')\n",
        "axes[0, 0].plot([y_test_original.min(), y_test_original.max()], \n",
        "                [y_test_original.min(), y_test_original.max()], \n",
        "                'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0, 0].set_xlabel('Actual Turbidity (NTU)', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Predicted Turbidity (NTU)', fontsize=12)\n",
        "axes[0, 0].set_title(f'Predicted vs Actual (R² = {r2:.3f})', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Time series plot: First 200 samples\n",
        "n_samples = min(200, len(y_test_original))\n",
        "axes[0, 1].plot(range(n_samples), y_test_original[:n_samples], label='Actual', linewidth=2, alpha=0.8)\n",
        "axes[0, 1].plot(range(n_samples), y_pred[:n_samples], label='Predicted', linewidth=2, alpha=0.8)\n",
        "axes[0, 1].set_xlabel('Sample Index', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Turbidity (NTU)', fontsize=12)\n",
        "axes[0, 1].set_title('Actual vs Predicted Turbidity (Sample)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual plot\n",
        "residuals = y_test_original - y_pred\n",
        "axes[1, 0].scatter(y_pred, residuals, alpha=0.5, s=20, c='darkgreen')\n",
        "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "axes[1, 0].set_xlabel('Predicted Turbidity (NTU)', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Residual (Actual - Predicted)', fontsize=12)\n",
        "axes[1, 0].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual distribution\n",
        "axes[1, 1].hist(residuals, bins=50, color='teal', edgecolor='white', alpha=0.8)\n",
        "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "axes[1, 1].axvline(x=np.mean(residuals), color='orange', linestyle='-', linewidth=2, label=f'Mean: {np.mean(residuals):.3f}')\n",
        "axes[1, 1].set_xlabel('Residual (NTU)', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 1].set_title('Residual Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/lstm_evaluation_plots.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Export Predictions for Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create predictions dataframe for Tableau dashboard\n",
        "# Get timestamps for test set predictions\n",
        "test_start_idx = train_size + val_size + SEQUENCE_LENGTH + FORECAST_HORIZON - 1\n",
        "timestamps = df_lstm['Timestamp'].iloc[test_start_idx:test_start_idx + len(y_test_original)].values\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Timestamp': timestamps,\n",
        "    'Actual_Turbidity': y_test_original,\n",
        "    'Predicted_Turbidity': y_pred,\n",
        "    'Prediction_Error': residuals,\n",
        "    'Absolute_Error': np.abs(residuals)\n",
        "})\n",
        "\n",
        "# Add derived columns\n",
        "predictions_df['Station'] = best_station\n",
        "predictions_df['Prediction_Date'] = pd.to_datetime(predictions_df['Timestamp']).dt.date\n",
        "\n",
        "# Calculate daily metrics\n",
        "daily_metrics = predictions_df.groupby('Prediction_Date').agg({\n",
        "    'Actual_Turbidity': 'mean',\n",
        "    'Predicted_Turbidity': 'mean',\n",
        "    'Absolute_Error': 'mean'\n",
        "}).reset_index()\n",
        "daily_metrics.columns = ['Date', 'Avg_Actual_Turbidity', 'Avg_Predicted_Turbidity', 'Avg_MAE']\n",
        "\n",
        "# Save predictions\n",
        "predictions_df.to_csv('../outputs/lstm_predictions.csv', index=False)\n",
        "daily_metrics.to_csv('../outputs/lstm_daily_metrics.csv', index=False)\n",
        "\n",
        "print(\"Saved predictions to: ../outputs/lstm_predictions.csv\")\n",
        "print(\"Saved daily metrics to: ../outputs/lstm_daily_metrics.csv\")\n",
        "print(f\"\\nPredictions shape: {predictions_df.shape}\")\n",
        "predictions_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implemented an LSTM deep learning model for water turbidity prediction:\n",
        "\n",
        "**Model Architecture:**\n",
        "- Built from scratch using TensorFlow/Keras (not pre-trained)\n",
        "- 2-layer stacked LSTM with dropout regularization\n",
        "- Input: 48-hour sequences of sensor data (Turbidity, Conductivity, Temperature)\n",
        "- Output: 24-hour ahead turbidity prediction\n",
        "\n",
        "**Key Results:**\n",
        "- The model learns temporal patterns in water quality data\n",
        "- Predictions can be used for early warning systems\n",
        "- Results exported for Tableau dashboard visualization\n",
        "\n",
        "**Next Steps:**\n",
        "- See Notebook 03 for Water Quality Classification model\n",
        "- Dashboard data ready for visualization in Tableau Public\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
