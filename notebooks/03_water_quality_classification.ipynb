{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Water Quality Classification Model\n",
        "\n",
        "## AAI-530 Final Project - Machine Learning Method 2\n",
        "\n",
        "This notebook implements a machine learning classification model to predict water quality status (Safe/Warning/Unsafe) based on sensor readings.\n",
        "\n",
        "**Objective**: Classify water quality status using multiple sensor inputs\n",
        "\n",
        "**Why Classification?**\n",
        "- Automated alerting system for water quality issues\n",
        "- Helps stakeholders make quick decisions\n",
        "- Can trigger real-time notifications in IoT systems\n",
        "\n",
        "**Model**: Random Forest Classifier\n",
        "- Handles non-linear relationships well\n",
        "- Provides feature importance insights\n",
        "- Robust to outliers and missing values\n",
        "- Different target variable from LSTM (status vs. turbidity value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                            accuracy_score, precision_score, recall_score, \n",
        "                            f1_score, roc_auc_score, roc_curve)\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "DATA_DIR = '../archive'\n",
        "\n",
        "def load_all_stations(data_dir):\n",
        "    \"\"\"Load and combine data from all monitoring stations.\"\"\"\n",
        "    all_data = []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(data_dir, filename)\n",
        "            station_name = filename.replace('_joined.csv', '').replace('_', ' ').title()\n",
        "            df = pd.read_csv(filepath)\n",
        "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "            df['Station'] = station_name\n",
        "            all_data.append(df)\n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "df = load_all_stations(DATA_DIR)\n",
        "print(f\"Loaded {len(df):,} records from {df['Station'].nunique()} stations\")\n",
        "print(f\"\\nAvailable columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create water quality classification labels\n",
        "def classify_water_quality(row):\n",
        "    \"\"\"\n",
        "    Classify water quality based on Australian water quality guidelines.\n",
        "    \n",
        "    Thresholds:\n",
        "    - Turbidity: Safe < 5 NTU, Warning 5-50 NTU, Unsafe > 50 NTU\n",
        "    - Conductivity: Safe < 30000 µS/cm, Warning 30000-50000, Unsafe > 50000\n",
        "    - Temperature: Safe 10-30°C, Warning 5-10 or 30-35°C, Unsafe < 5 or > 35°C\n",
        "    \n",
        "    Returns: 'Safe', 'Warning', or 'Unsafe'\n",
        "    \"\"\"\n",
        "    unsafe_count = 0\n",
        "    warning_count = 0\n",
        "    \n",
        "    # Check Turbidity\n",
        "    if pd.notna(row.get('Turbidity')):\n",
        "        if row['Turbidity'] > 50:\n",
        "            unsafe_count += 1\n",
        "        elif row['Turbidity'] > 5:\n",
        "            warning_count += 1\n",
        "    \n",
        "    # Check Conductivity\n",
        "    if pd.notna(row.get('Conductivity')):\n",
        "        if row['Conductivity'] > 50000:\n",
        "            unsafe_count += 1\n",
        "        elif row['Conductivity'] > 30000:\n",
        "            warning_count += 1\n",
        "    \n",
        "    # Check Temperature\n",
        "    if pd.notna(row.get('Temp')):\n",
        "        if row['Temp'] < 5 or row['Temp'] > 35:\n",
        "            unsafe_count += 1\n",
        "        elif row['Temp'] < 10 or row['Temp'] > 30:\n",
        "            warning_count += 1\n",
        "    \n",
        "    if unsafe_count >= 1:\n",
        "        return 'Unsafe'\n",
        "    elif warning_count >= 1:\n",
        "        return 'Warning'\n",
        "    else:\n",
        "        return 'Safe'\n",
        "\n",
        "# Apply classification\n",
        "print(\"Creating water quality labels...\")\n",
        "df['Quality_Status'] = df.apply(classify_water_quality, axis=1)\n",
        "\n",
        "# Display distribution\n",
        "print(\"\\nWater Quality Distribution:\")\n",
        "print(df['Quality_Status'].value_counts())\n",
        "print(f\"\\nPercentages:\")\n",
        "print((df['Quality_Status'].value_counts(normalize=True) * 100).round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for classification\n",
        "# Add time-based features\n",
        "df['Hour'] = df['Timestamp'].dt.hour\n",
        "df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
        "df['Month'] = df['Timestamp'].dt.month\n",
        "df['Year'] = df['Timestamp'].dt.year\n",
        "df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Cyclical encoding\n",
        "df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
        "df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
        "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "\n",
        "print(\"Added time-based features!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Features for Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for classification\n",
        "# Use sensor readings and time features (NOT the raw thresholding variables used in labels)\n",
        "potential_features = ['Conductivity', 'NO3', 'Temp', 'Turbidity', 'Level', 'Q',\n",
        "                      'Hour_sin', 'Hour_cos', 'Month_sin', 'Month_cos', 'IsWeekend']\n",
        "\n",
        "# Check which features are available and have sufficient data\n",
        "feature_cols = []\n",
        "for col in potential_features:\n",
        "    if col in df.columns:\n",
        "        non_null_pct = df[col].notna().sum() / len(df) * 100\n",
        "        if non_null_pct > 10:  # Keep features with >10% data\n",
        "            feature_cols.append(col)\n",
        "            print(f\"  {col}: {non_null_pct:.1f}% non-null\")\n",
        "\n",
        "print(f\"\\nSelected {len(feature_cols)} features for classification\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create feature matrix and target vector\n",
        "# Drop rows with missing values in selected features\n",
        "df_model = df[feature_cols + ['Quality_Status']].dropna()\n",
        "\n",
        "print(f\"Dataset for modeling: {len(df_model):,} samples\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_model[feature_cols].values\n",
        "y = df_model['Quality_Status'].values\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\nClass mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
        "print(f\"\\nClass distribution in final dataset:\")\n",
        "for cls, count in zip(*np.unique(y_encoded, return_counts=True)):\n",
        "    print(f\"  {label_encoder.inverse_transform([cls])[0]}: {count:,} ({count/len(y_encoded)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y_encoded  # Maintain class distribution\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data Split Summary:\")\n",
        "print(f\"  Training set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"  Test set: {X_test.shape[0]:,} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,          # Number of trees\n",
        "    max_depth=15,              # Maximum depth of trees\n",
        "    min_samples_split=5,       # Minimum samples to split node\n",
        "    min_samples_leaf=2,        # Minimum samples in leaf node\n",
        "    class_weight='balanced',   # Handle class imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1                  # Use all CPU cores\n",
        ")\n",
        "\n",
        "print(\"Training Random Forest Classifier...\")\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Cross-validation score\n",
        "cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
        "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "y_pred_proba = rf_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Classification Metrics (Test Set):\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Raw counts\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "axes[0].set_xlabel('Predicted', fontsize=12)\n",
        "axes[0].set_ylabel('Actual', fontsize=12)\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Normalized\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "axes[1].set_xlabel('Predicted', fontsize=12)\n",
        "axes[1].set_ylabel('Actual', fontsize=12)\n",
        "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/classification_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = plt.cm.RdYlGn(feature_importance['Importance'] / feature_importance['Importance'].max())\n",
        "bars = ax.barh(feature_importance['Feature'], feature_importance['Importance'], color=colors)\n",
        "ax.set_xlabel('Importance', fontsize=12)\n",
        "ax.set_ylabel('Feature', fontsize=12)\n",
        "ax.set_title('Feature Importance for Water Quality Classification', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add value labels\n",
        "for bar, imp in zip(bars, feature_importance['Importance']):\n",
        "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "            f'{imp:.3f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/classification_feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 Most Important Features:\")\n",
        "for _, row in feature_importance.tail(5).iloc[::-1].iterrows():\n",
        "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Export Results for Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create classification results dataframe for Tableau dashboard\n",
        "classification_results = pd.DataFrame({\n",
        "    'Actual_Class': label_encoder.inverse_transform(y_test),\n",
        "    'Predicted_Class': label_encoder.inverse_transform(y_pred),\n",
        "    'Safe_Probability': y_pred_proba[:, label_encoder.transform(['Safe'])[0]],\n",
        "    'Warning_Probability': y_pred_proba[:, label_encoder.transform(['Warning'])[0]],\n",
        "    'Unsafe_Probability': y_pred_proba[:, label_encoder.transform(['Unsafe'])[0]] if 'Unsafe' in label_encoder.classes_ else 0\n",
        "})\n",
        "\n",
        "# Add feature values for context\n",
        "for i, col in enumerate(feature_cols):\n",
        "    classification_results[col] = X_test[:, i]\n",
        "\n",
        "# Add correct/incorrect flag\n",
        "classification_results['Correct_Prediction'] = (classification_results['Actual_Class'] == \n",
        "                                                  classification_results['Predicted_Class']).astype(int)\n",
        "\n",
        "# Save to CSV\n",
        "classification_results.to_csv('../outputs/classification_results.csv', index=False)\n",
        "print(\"Saved classification results to: ../outputs/classification_results.csv\")\n",
        "print(f\"Results shape: {classification_results.shape}\")\n",
        "classification_results.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary metrics for dashboard\n",
        "summary_metrics = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Total Samples', \n",
        "               'Safe Count', 'Warning Count', 'Unsafe Count'],\n",
        "    'Value': [accuracy, precision, recall, f1, len(y_test),\n",
        "              (y_pred == label_encoder.transform(['Safe'])[0]).sum(),\n",
        "              (y_pred == label_encoder.transform(['Warning'])[0]).sum(),\n",
        "              (y_pred == label_encoder.transform(['Unsafe'])[0]).sum() if 'Unsafe' in label_encoder.classes_ else 0]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_metrics)\n",
        "summary_df.to_csv('../outputs/classification_summary.csv', index=False)\n",
        "print(\"Saved classification summary to: ../outputs/classification_summary.csv\")\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "joblib.dump(rf_model, '../models/random_forest_classifier.joblib')\n",
        "joblib.dump(scaler, '../models/scaler.joblib')\n",
        "joblib.dump(label_encoder, '../models/label_encoder.joblib')\n",
        "print(\"Saved model and preprocessing objects to ../models/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
